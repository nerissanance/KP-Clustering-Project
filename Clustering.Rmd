---
title: "Clustering"
author: "Amy Alabaster and Nerissa Nance"
date: "3/18/2019"
output:
  html_document: 
    toc: true
  pdf_document: default
  word_document: default 
---

#Hierarchical clustering
```{r setup, include=FALSE}

library(rio)
library(mclust)
library(tidyverse)
library(factoextra)
library(umap)
library(Rtsne)

data <- import("./data/heart.csv")
head(data)
names(data)

#remove the outcome 

```
###T-sne
```{r}
#run tsne
set.seed(7)  
tsne_model = Rtsne(as.matrix(data), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.25, dims=2, max_iter=5000)

#get data frame
tsne_dat = as.data.frame(tsne_model$Y)  
tsne_dat$target=as.factor(data$target)

#plotting
ggplot(tsne_dat, aes(x=V1, y=V2, color=target)) +  
  geom_point(size=0.25) +
  xlab("") + ylab("") +
  ggtitle("t-SNE") +
  theme_light(base_size=20) +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank()) 
```

###Data prep and distance matrix

Need to create a distance matrix from scaled variables 
```{r}

data_s <- scale(data)
data_dist <- dist(data_s)
```

###Model fitting and dendrogram

```{r}
data_m <- as.matrix(data)

hc<-hclust(data_dist, "average")
#other options: complete or average
plot(hc, hang=-1, main="Patient characteristics", ylab=NULL)

#cutree for plotting in tsne
tsne_dat$cl_hierarchical = factor(cutree(hc, k=3))

#plot on tsne (currently looks awful - but code is here)
ggplot(tsne_dat, aes_string(x="V1", y="V2", color="cl_hierarchical")) +
  geom_point(size=0.25) +
  guides(colour=guide_legend(override.aes=list(size=6))) +
  xlab("") + ylab("") +
  ggtitle("Hierarchical on t-SNE") +
  theme_light(base_size=20) +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        legend.direction = "horizontal", 
        legend.position = "bottom",
        legend.box = "horizontal") + 
    scale_colour_brewer(palette = "Set1") 

```

###Cutree
(add here about how to make meangingful groups from the hclust results)

-----------


#K-means clustering

###Data prep
Aside from general cleaning, need to standardize the data so that the variables are comparable:
```{r}
data_s <- scale(data_m)
head(data_s, n=5)
```

###Decide on a distance matrix
There are lot of options, just as there are for heirarchical. The default is Euclidian.
(list here, link to website with more details)
```{r}
dist_m <- dist(data_s)

fviz_dist(dist_m)

```

### Clustering 

Steps from the dlab[https://github.com/dlab-berkeley/MachineLearningWG/blob/master/Fall2018/2-sep19-k-means/k-means-ucr.Rmd]:

* 1. Specify the number of clusters (K) to be created (by the analyst)
* 2. Select randomly k objects from the data set as the initial cluster centers or means
* 3. Assigns each observation to their closest centroid, based on the Euclidean distance between the object and the centroid
* 4. For each of the k clusters update the cluster centroid by calculating the new mean values of all the data points in the cluster. The centroid of a Kth cluster is a vector of length *p* containing the means of all variables for the observations
in the kth cluster; *p* is the number of variables.
* 5. Iteratively minimize the total within sum of square (Eq. 7). That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. By default, the R software uses 10 as the default value
for the maximum number of iterations.

```{r}

set.seed(100)
km<-kmeans(data_s, centers = 2, nstart = 25)

```


Objects outputted: 
* `cluster`: A vector of integers (from 1:k) indicating the cluster to which each point is allocated.

* `centers`: A matrix of cluster centers.
* `totss`: The total sum of squares.
* `withinss`: Vector of within-cluster sum of squares, one component per cluster.
* `tot.withinss`: Total within-cluster sum of squares, i.e. sum(withinss).
* `betweenss`: The between-cluster sum of squares, i.e. $totss-tot.withinss$.
* `size`: The number of points in each cluster.

###Visualizing the results

With package factoextra: 

```{r}

fviz_cluster(km, data = data_s)
 
#kmens with tsne
tsne_dat$cl_kmeans = factor(km$cluster)
ggplot(tsne_dat, aes_string(x="V1", y="V2", color="cl_kmeans")) +
  geom_point(size=0.25) +
  guides(colour=guide_legend(override.aes=list(size=6))) +
  xlab("") + ylab("") +
  ggtitle("Kmeans plotted using t-SNE") +
  theme_light(base_size=20) +
  theme(axis.text.x=element_blank(),
        axis.text.y=element_blank(),
        legend.direction = "horizontal", 
        legend.position = "bottom",
        legend.box = "horizontal") + 
    scale_colour_brewer(palette = "Set1") 
```




####Trying the same cluster analyses after dimension reduction

Dimension reduction and visualization with packages like UMAP are one way to reduce your data.

(add here: what is the value added of performing cluster analysis after dimension reduction..... maybe not necessary for out dataset since it's small, but helpful for larger datasets, and also always helpful for visualizations.)

```{r}

umap_output <- umap(data)

```


####Looking at covariates by cluster results 

(add here a table 1-like thing that looks at the demographics by cluster)
```{r}


```

####Extra credit: calculating optimal number of clusters 

Things to research and maybe add later: Elbow method, Silhoette method, Gap Statistic.

Hopefully add these if there's time.