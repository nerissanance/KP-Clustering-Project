---
title: "Clustering"
author: "Amy Alabaster and Nerissa Nance"
date: "3/18/2019"
output:
  pdf_document: default
  html_document: default
toc: yes
---

This report shows two possible clustering analyses: hierarchical and k-means.

```{r setup, include=FALSE}

library(rio)
library(mclust)
library(tidyverse)
library(factoextra)
library(umap)

data <- import("./data/heart.csv")
head(data)
names(data)

#remove the outcome 

```

#Hierarchical clustering

###Data prep and distance matrix

Need to create a distance matrix from caled variables 
```{r}
data_dist <- dist(data_m)
```

###Model fitting and dendrogram

```{r}
data_m <- as.matrix(data)

hc<-hclust(data_dist, "average")
#other options: complete or average
plot(hc)
plot(hc, hang=-1, main="Patient characteristics", ylab=NULL)

```

###Cutree
(add here about how to make meangingful gropus from the hclust results)

-----------


#K-means clustering

###Data prep
Aside from general cleaning, need to standardize the data so that the variables are comparable:
```{r}
data_s <- scale(data_m)
head(data_s, n=5)
```

###Decide on a distance matrix
There are lot of options, just as there are for heirarchical. The default is Euclidian.
(list here, link to website with more details)
```{r}
dist_m <- dist(data_s)

fviz_dist(dist_m)

```

###Step 3: Clustering

Steps from the dlab[https://github.com/dlab-berkeley/MachineLearningWG/blob/master/Fall2018/2-sep19-k-means/k-means-ucr.Rmd]:

* 1. Specify the number of clusters (K) to be created (by the analyst)
* 2. Select randomly k objects from the data set as the initial cluster centers or means
* 3. Assigns each observation to their closest centroid, based on the Euclidean distance between the object and the centroid
* 4. For each of the k clusters update the cluster centroid by calculating the new mean values of all the data points in the cluster. The centroid of a Kth cluster is a vector of length *p* containing the means of all variables for the observations
in the kth cluster; *p* is the number of variables.
* 5. Iteratively minimize the total within sum of square (Eq. 7). That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. By default, the R software uses 10 as the default value
for the maximum number of iterations.

```{r}

set.seed(100)
km<-kmeans(data_s, centers = 2, nstart = 25)

```


Objects outputted: 
* `cluster`: A vector of integers (from 1:k) indicating the cluster to which each point is allocated.

* `centers`: A matrix of cluster centers.
* `totss`: The total sum of squares.
* `withinss`: Vector of within-cluster sum of squares, one component per cluster.
* `tot.withinss`: Total within-cluster sum of squares, i.e. sum(withinss).
* `betweenss`: The between-cluster sum of squares, i.e. $totss-tot.withinss$.
* `size`: The number of points in each cluster.

###Visualizing the results

With package factoextra: 

```{r}

fviz_cluster(km, data = data_s)
 

```

Dimension reduction and visualization with 


####Trying the same cluster analyses after dimension reduction

```{r}

umap_output <- umap(data)

```


####Looking at covariates by cluster results 

()
```{r}


```

####Extra credit: calculating optimal 

Things to research and maybe add later: Elbow method, Silhoette method, Gap Statistic.

Hopefully add these if there's time 